---
title: "Topic Modeling Script"
author: "Enrique Nusi"
date: "5/4/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## STEP 1: Load libraries and textOriginal data from final sampled data, add additional missing stopwords as needed.

```{r cars}

library(tidyverse)
library(stopwords)
library(tm)
library(topicmodels)
library(ldatuning)
library(parallel)
library(LDAvis)
library(stringi)
library(quanteda)
library(quanteda.textstats)
library(seededlda)

data <- read_csv("dataset_MM_DD_YY.csv") #read in your data from .csv
data <- as.matrix(data$textWords) #convert just the text column to a matrix array for conversion into a Corpus/corpus object

missingStopWords <- c("the", "just", "like", "can", "got", "us", "also", "oh", "even", "say", "said", "based", "yeah", "yet", "go", "get") #just add/remove them as needed

```


## STEP 2: Pre-process raw data for conversion to corpus/Corpus format then generate quanteda and tm corpus objects.

```{r, echo=FALSE}

data <- tolower(data)  #force to lowercase
data[stri_count(data, regex="\\S+") < 8] = ""
data <- gsub("'", "", data)  #remove apostrophes
data <-
  gsub("[[:punct:]]", " ", data)  #replace punctuation with space
data <-
  gsub("[[:cntrl:]]", " ", data)  #replace control characters with space
data <-
  gsub("[[:digit:]]", "", data)  #remove digits
data <-
  gsub("^[[:space:]]+", "", data) #remove whitespace at beginning of documents
data <-
  gsub("[[:space:]]+$", "", data) #remove whitespace at end of documents
data <- stripWhitespace(data)

corp <- corpus(data) #quanteda-compatible corpus object (lower-case c) for general analysis
exploratory_corp <- Corpus(VectorSource(data)) #tm-compatible Corpus object (upper-case C) for topic number discovery

```


## STEP 3: Tokenize and tidy quanteda corpus object, generate colocations for accuracy, and generate quanteda and tm objects for conversion into dfm/dtm objects.

```{r, echo=FALSE}

quantok <- corp %>% #tidy your corpus object for conversion to document-feature matrix and document term matrix
  tokens() %>%
  tokens_tolower() %>%
  tokens_remove(pattern=c(stopwords::stopwords(), missingStopWords), padding=TRUE) #removes stop words based on the stopwords package and the user-defined missingStopWords list

corpcolo <- textstat_collocations(quantok, size=2, min_count=200) #creates a list of terms by calculating when collocated words appear at least as many times as indicated in min_count=x

quantok <- tokens_compound(quantok, corpcolo) #final corpus for conversion into dfm/dtm (quanteda function)
tmtok <- Corpus(VectorSource(quantok)) #convert quanteda tokens with colocations to tm Corpus

```


## STEP 4: Generate quanteda-compatible DFM for textstat_frequency usage; process it for use with the LDA function.

```{r, echo=FALSE}

dfm1 <- quantok %>% 
  tokens_remove("") %>% #removes empty space
  dfm() %>% #converts quanteda corpus to document-feature matrix
  dfm_trim(min_termfreq=2, docfreq_type="prop") #select how often a term should appear for it to be included in analysis and how many documents should include at least one topic to be counted

raw.sum <- apply(dfm1,1,FUN=sum) #raw sums to count up zeroes
dfm1 <- dfm1[raw.sum!=0,] #remove rows with zeroes to make the dfm compatible with the LDA() function

```


## STEP 5: Generate tm-compatible DTM object and process it for use with the FindTopicsNumber function.

```{r, echo=FALSE}

dtm1 <- DocumentTermMatrix(tmtok, control=list( 
  language="english",  #language of data set
  stopwords=c(stopwords::stopwords(), missingStopWords))) #removes stop words based on the stopwords package and the user-defined missingStopWords list
ui = unique(dtm1$i)
dtm1 = dtm1[ui,]
#raw.sum <- apply(dtm1,1,FUN=sum) #raw sums to count up zeroes
#dtm1 <- dtm1[raw.sum!=0,] #remove rows with zeroes to match the LDA-compatible object

```


## STEP 6: Generate and interpret graph to determine the number (K) of topics to examine--somewhere between the max of Griffiths and the min of Arun (add the numbers together and divide by 2 to get a rough estimate of your final K value).  Adjust top=seq() numbers to zoom in/out as needed.

```{r, echo=FALSE}

poss_k <- FindTopicsNumber(dtm1, #OBJECT MUST BE DTM COMPATIBLE WITH TM PACKAGE--CANNOT BE QUANTEDA-GENERATED DFM
                           topics=seq(1,100, by=10), #lower-limit to upper-limit of possible topics for analysis in increments of by=x; if encountering an error, try lowering the upper-limit or increasing the by=x
                           metrics=c( "Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"), #generates 4 line plots from which you will determine your K value
                           mc.cores=5) #number of processor cores to be used--if using a multi-core processor, figure out the number of cores you have and subtract one then enter that number

FindTopicsNumber_plot(poss_k) #generate a plot visualization of the above object

```


## STEP 7: Enter your K number and run to generate an LDA model, then generate posterior data for suplementary analysis as needed.

```{r, echo=FALSE}

K <- 50 #the number of topics to model
lda1 <- LDA(dtm1, K, method="Gibbs") #creates an LDA model based on the dfm1 object in STEP 2 with K-number of topics analyzed

post_results <- posterior(lda1) #posterior object for later analysis tbd
post_terms <- posterior(lda1)$terms #numerical values for terms/documents frequency
post_topics <- posterior(lda1)$topics

```


## STEP 8: Generate a list of 1-, 2-, and 3-word colocations for glossary research and create a .csv; these can go in the raw data folder for the appropriate report cycle.

```{r, echo=FALSE}

print_1_word_colo <- textstat_frequency(dfm1, n=400)
print_3_word_colo <- textstat_collocations(quantok, size=3, min_count=50) #creates a list of terms by calculating when 3 colocated words appear at least as many times as indicated in min_count=x


write_csv(print_1_word_colo, file="data_1word.csv")
write_csv(corpcolo, file="data_2word.csv")
write_csv(print_3_word_colo, file="data_3word")

```


## STEP 9: Generate and tidy a topic terms list, then create a .csv; this can go in the raw data folder for the appropriate report cycle.

```{r, echo=FALSE}

topic_terms <- as.matrix(topicmodels::terms(lda1, 20)) #Generates a topic model with K number of topics displaying the top 20 words in those topics.
topic_names <- apply(topic_terms, 2, paste, collapse=" ") #Generates a potential list of names for each topic based on the top 2 most prevalent words in the model--will not be attached to the final topic model .csv, but you can generate your own .csv if you feel it's important enough

topic_model <- as.data.frame(topic_terms)
write_csv(topic_model, file="topic_model.csv")

```


Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
